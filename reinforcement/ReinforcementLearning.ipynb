{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReinforcementLearning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "CYatltmqFJ3Y",
        "colab_type": "code",
        "outputId": "ffa9bbb2-9765-4a3d-ab5d-f90269f5f6e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install gym[atari]\n",
        "!pip install gym[Box2D]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.10.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: atari_py>=0.1.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.1.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.0.0)\n",
            "Requirement already satisfied: PyOpenGL in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2018.11.29)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->gym[atari]) (0.46)\n",
            "Requirement already satisfied: gym[Box2D] in /usr/local/lib/python3.6/dist-packages (0.10.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[Box2D]) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[Box2D]) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[Box2D]) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[Box2D]) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[Box2D]) (1.3.2)\n",
            "Requirement already satisfied: box2d-py>=2.3.5 in /usr/local/lib/python3.6/dist-packages (from gym[Box2D]) (2.3.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[Box2D]) (2018.11.29)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[Box2D]) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[Box2D]) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[Box2D]) (2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[Box2D]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q9-ddacAcoBe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import gym\n",
        "# import universe\n",
        "from skimage import io\n",
        "from skimage.color import rgb2gray\n",
        "from skimage import transform\n",
        "from collections import deque# Ordered collection with ends\n",
        "import tensorflow as tf\n",
        "import random\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rKsFfdCIEDj9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Frame_Preprocessor:\n",
        "  def __init__(self,crop=True,convert_to_grayscale=True,normalize=True,coords={},resize=True,resize_y,resize_x):\n",
        "    \n",
        "    self.crop=crop\n",
        "    self.top_left_x=coords.get('top_left_x',0)\n",
        "    self.top_left_y=coords('top_left_y',0)\n",
        "    self.bottom_right_x=coords.get('bottom_right_x',-1)\n",
        "    self.bottom_right_y=coords.get('bottom_right_y',-1)\n",
        "    self.convert_to_grayscale=convert_to_grayscale\n",
        "    self.normalize=normalize\n",
        "    self.resize=resize\n",
        "    self.resize_x=resize_x\n",
        "    self.resize_y=resize_y\n",
        "    \n",
        "  def process_frame(self,frame):\n",
        "    if self.convert_to_grayscale:\n",
        "      frame=rgb2gray(frame)\n",
        "    \n",
        "    if self.crop:\n",
        "      frame=frame[self.top_left_y:self.bottom_right_y,self.top_left_x:self.bottom_right_x]\n",
        "      \n",
        "    if self.normalize:\n",
        "      frame=frame/255.0\n",
        "      \n",
        "    if self.resize:\n",
        "      frame=transform.resize(frame, [self.resize_y,resize_x])\n",
        "      \n",
        "    return frame\n",
        "      \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "19-n_0g7IyNp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "  \n",
        "  def __init__(self,env_name,FramePreprocessor):\n",
        "    self.env = gym.make(env_name)\n",
        "    print(\"The size of our frame is: \", self.env.observation_space)\n",
        "    print(\"The action size is : \", self.env.action_space.n)\n",
        "    self.possible_actions=env.action_space.n\n",
        "    self.curr_state=None\n",
        "    self.F_Processor=FramePreprocessor\n",
        "    self.reset()\n",
        "    \n",
        "  def step(self,action):\n",
        "    next_state, reward, done,info=self.env.step(action)\n",
        "    self.curr_state=self.F_Preprocessor.process_frame(next_state)\n",
        "    return self.curr_state, reward, done\n",
        "  \n",
        "  def reset(self):\n",
        "    self.curr_state=self.env.reset()\n",
        "    \n",
        "  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W10kG56l3x7s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_frame(frame):\n",
        "    # Greyscale frame \n",
        "    gray = rgb2gray(frame)\n",
        "    \n",
        "    # Crop the screen (remove the part below the player)\n",
        "    # [Up: Down, Left: right]\n",
        "    cropped_frame = gray[8:-12,4:-12]\n",
        "    \n",
        "    # Normalize Pixel Values\n",
        "    normalized_frame = cropped_frame/255.0\n",
        "    \n",
        "    # Resize\n",
        "    # Thanks to Miko≈Çaj Walkowiak\n",
        "    preprocessed_frame = transform.resize(normalized_frame, [110,84])\n",
        "    \n",
        "    return preprocessed_frame # 110x84x1 fram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o9AZrOlz4w4C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "72cfebb0-d4b1-4b70-e4c3-4ea6b439742d"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('SpaceInvaders-v0')\n",
        "\n",
        "print(\"The size of our frame is: \", env.observation_space)\n",
        "print(\"The action size is : \", env.action_space.n)\n",
        "\n",
        "# Here we create an hot encoded version of our actions\n",
        "# possible_actions = [[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0]...]\n",
        "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The size of our frame is:  Box(210, 160, 3)\n",
            "The action size is :  6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LgF6_NjR4_ZR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stack_size = 4 # We stack 4 frames\n",
        "\n",
        "# Initialize deque with zero-images one array for each image\n",
        "stacked_frames  =  deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    # Preprocess frame\n",
        "    frame = preprocess_frame(state)\n",
        "    \n",
        "    if is_new_episode:\n",
        "        # Clear our stacked_frames\n",
        "        stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "        \n",
        "        # Because we're in a new episode, copy the same frame 4x\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        \n",
        "        # Stack the frames\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "        \n",
        "    else:\n",
        "        # Append frame to deque, automatically removes the oldest frame\n",
        "        stacked_frames.append(frame)\n",
        "\n",
        "        # Build the stacked state (first dimension specifies different frames)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2) \n",
        "    \n",
        "    return stacked_state, stacked_frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J9yjb1wa52UT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### MODEL HYPERPARAMETERS\n",
        "state_size = [110, 84, 4]      # Our input is a stack of 4 frames hence 110x84x4 (Width, height, channels) \n",
        "action_size = env.action_space.n # 8 possible actions\n",
        "learning_rate =  0.00025      # Alpha (aka learning rate)\n",
        "\n",
        "### TRAINING HYPERPARAMETERS\n",
        "total_episodes = 50            # Total episodes for training\n",
        "max_steps = 50000              # Max possible steps in an episode\n",
        "batch_size = 64                # Batch size\n",
        "\n",
        "# Exploration parameters for epsilon greedy strategy\n",
        "explore_start = 1.0            # exploration probability at start\n",
        "explore_stop = 0.01            # minimum exploration probability \n",
        "decay_rate = 0.00001           # exponential decay rate for exploration prob\n",
        "\n",
        "# Q learning hyperparameters\n",
        "gamma = 0.9                    # Discounting rate\n",
        "\n",
        "### MEMORY HYPERPARAMETERS\n",
        "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
        "memory_size = 1000000          # Number of experiences the Memory can keep\n",
        "\n",
        "### PREPROCESSING HYPERPARAMETERS\n",
        "stack_size = 4                 # Number of frames stacked\n",
        "\n",
        "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
        "training = True\n",
        "\n",
        "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
        "episode_render = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_hL9L-RZ6AMU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQNetwork:\n",
        "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        with tf.variable_scope(name):\n",
        "            # We create the placeholders\n",
        "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
        "            # [None, 84, 84, 4]\n",
        "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
        "            self.actions_ = tf.placeholder(tf.float32, [None, self.action_size], name=\"actions_\")\n",
        "            \n",
        "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
        "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
        "            \n",
        "            \"\"\"\n",
        "            First convnet:\n",
        "            CNN\n",
        "            ELU\n",
        "            \"\"\"\n",
        "            # Input is 110x84x4\n",
        "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
        "                                         filters = 32,\n",
        "                                         kernel_size = [8,8],\n",
        "                                         strides = [4,4],\n",
        "                                         padding = \"VALID\",\n",
        "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "                                         name = \"conv1\")\n",
        "            \n",
        "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
        "            \n",
        "            \"\"\"\n",
        "            Second convnet:\n",
        "            CNN\n",
        "            ELU\n",
        "            \"\"\"\n",
        "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
        "                                 filters = 64,\n",
        "                                 kernel_size = [4,4],\n",
        "                                 strides = [2,2],\n",
        "                                 padding = \"VALID\",\n",
        "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "                                 name = \"conv2\")\n",
        "\n",
        "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\")            \n",
        "            \n",
        "            \"\"\"\n",
        "            Third convnet:\n",
        "            CNN\n",
        "            ELU\n",
        "            \"\"\"\n",
        "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
        "                                 filters = 64,\n",
        "                                 kernel_size = [3,3],\n",
        "                                 strides = [2,2],\n",
        "                                 padding = \"VALID\",\n",
        "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "                                 name = \"conv3\")\n",
        "\n",
        "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
        "            \n",
        "            self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
        "            \n",
        "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
        "                                  units = 512,\n",
        "                                  activation = tf.nn.elu,\n",
        "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                                name=\"fc1\")\n",
        "            \n",
        "            self.output = tf.layers.dense(inputs = self.fc, \n",
        "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                                          units = self.action_size, \n",
        "                                        activation=None)\n",
        "            \n",
        "\n",
        "  \n",
        "            # Q is our predicted Q value.\n",
        "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_))\n",
        "            \n",
        "            # The loss is the difference between our predicted Q_values and the Q_target\n",
        "            # Sum(Qtarget - Q)^2\n",
        "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
        "            \n",
        "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cuOE8t_-6KyU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Reset the graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Instantiate the DQNetwork\n",
        "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o5nNFkge6b2C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Memory():\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen = max_size)\n",
        "    \n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        buffer_size = len(self.buffer)\n",
        "        index = np.random.choice(np.arange(buffer_size),\n",
        "                                size = batch_size,\n",
        "                                replace = False)\n",
        "        \n",
        "        return [self.buffer[i] for i in index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zai_4jX36bsQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "7155b90a-53cd-4d0c-d5a5-f4cc5b9c9b32"
      },
      "cell_type": "code",
      "source": [
        "# Instantiate memory\n",
        "memory = Memory(max_size = memory_size)\n",
        "for i in range(pretrain_length):\n",
        "    # If it's the first step\n",
        "    if i == 0:\n",
        "        state = env.reset()\n",
        "        \n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "        \n",
        "    # Get the next_state, the rewards, done by taking a random action\n",
        "    choice = random.randint(1,len(possible_actions))-1\n",
        "    action = possible_actions[choice]\n",
        "#     print (action)\n",
        "    next_state, reward, done, _ = env.step(np.argmax(action))\n",
        "    \n",
        "    #env.render()\n",
        "    \n",
        "    # Stack the frames\n",
        "    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "    \n",
        "    \n",
        "    # If the episode is finished (we're dead 3x)\n",
        "    if done:\n",
        "        # We finished the episode\n",
        "        next_state = np.zeros(state.shape)\n",
        "        \n",
        "        # Add experience to memory\n",
        "        memory.add((state, action, reward, next_state, done))\n",
        "        \n",
        "        # Start a new episode\n",
        "        state = env.reset()\n",
        "        \n",
        "        # Stack the frames\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "        \n",
        "    else:\n",
        "        # Add experience to memory\n",
        "        memory.add((state, action, reward, next_state, done))\n",
        "        \n",
        "        # Our new state is now the next_state\n",
        "        state = next_state"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "sRtUXV7g7lyD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Setup TensorBoard Writer\n",
        "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
        "\n",
        "## Losses\n",
        "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
        "\n",
        "write_op = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DW1itkTR6biq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This function will do the part\n",
        "With œµœµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
        "\"\"\"\n",
        "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
        "    ## EPSILON GREEDY STRATEGY\n",
        "    # Choose action a from state s using epsilon greedy.\n",
        "    ## First we randomize a number\n",
        "    exp_exp_tradeoff = np.random.rand()\n",
        "\n",
        "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
        "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
        "    \n",
        "    if (explore_probability > exp_exp_tradeoff):\n",
        "        # Make a random action (exploration)\n",
        "        choice = random.randint(1,len(possible_actions))-1\n",
        "        action = possible_actions[choice]\n",
        "        \n",
        "    else:\n",
        "        # Get action from Q-network (exploitation)\n",
        "        # Estimate the Qs values state\n",
        "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
        "        \n",
        "        # Take the biggest Q value (= the best action)\n",
        "        choice = np.argmax(Qs)\n",
        "        action = possible_actions[choice]\n",
        "                \n",
        "                \n",
        "    return action, explore_probability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h310ZtBQ6bAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1153
        },
        "outputId": "224a96b9-e30b-44a2-9656-8f201651651c"
      },
      "cell_type": "code",
      "source": [
        "# Saver will help us to save our model\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "if training == True:\n",
        "    with tf.Session() as sess:\n",
        "        # Initialize the variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # Initialize the decay rate (that will use to reduce epsilon) \n",
        "        decay_step = 0\n",
        "        \n",
        "        for episode in range(total_episodes):\n",
        "            # Set step to 0\n",
        "            step = 0\n",
        "            \n",
        "            # Initialize the rewards of the episode\n",
        "            episode_rewards = []\n",
        "            \n",
        "            # Make a new episode and observe the first state\n",
        "            state = env.reset()\n",
        "            \n",
        "            # Remember that stack frame function also call our preprocess function.\n",
        "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "            \n",
        "            while step < max_steps:\n",
        "                step += 1\n",
        "                \n",
        "                #Increase decay_step\n",
        "                decay_step +=1\n",
        "                \n",
        "                # Predict the action to take and take it\n",
        "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
        "                \n",
        "                #Perform the action and get the next_state, reward, and done information\n",
        "                next_state, reward, done, _ = env.step(np.argmax(action))\n",
        "                \n",
        "                if episode_render:\n",
        "                    env.render()\n",
        "                \n",
        "                # Add the reward to total reward\n",
        "                episode_rewards.append(reward)\n",
        "                \n",
        "                # If the game is finished\n",
        "                if done:\n",
        "                    # The episode ends so no next state\n",
        "                    next_state = np.zeros((110,84), dtype=np.int)\n",
        "                    \n",
        "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "\n",
        "                    # Set step = max_steps to end the episode\n",
        "                    step = max_steps\n",
        "\n",
        "                    # Get the total reward of the episode\n",
        "                    total_reward = np.sum(episode_rewards)\n",
        "\n",
        "                    print('Episode: {}'.format(episode),\n",
        "                                  'Total reward: {}'.format(total_reward),\n",
        "                                  'Explore P: {:.4f}'.format(explore_probability),\n",
        "                                'Training Loss {:.4f}'.format(loss))\n",
        "\n",
        "#                     rewards_list.append((episode, total_reward))\n",
        "\n",
        "                    # Store transition <st,at,rt+1,st+1> in memory D\n",
        "                    memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "                else:\n",
        "                    # Stack the frame of the next_state\n",
        "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "                \n",
        "                    # Add experience to memory\n",
        "                    memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "                    # st+1 is now our current state\n",
        "                    state = next_state\n",
        "                    \n",
        "\n",
        "                ### LEARNING PART            \n",
        "                # Obtain random mini-batch from memory\n",
        "                batch = memory.sample(batch_size)\n",
        "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
        "                actions_mb = np.array([each[1] for each in batch])\n",
        "                rewards_mb = np.array([each[2] for each in batch]) \n",
        "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
        "                dones_mb = np.array([each[4] for each in batch])\n",
        "\n",
        "                target_Qs_batch = []\n",
        "\n",
        "                # Get Q values for next_state \n",
        "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
        "                \n",
        "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
        "                for i in range(0, len(batch)):\n",
        "                    terminal = dones_mb[i]\n",
        "\n",
        "                    # If we are in a terminal state, only equals reward\n",
        "                    if terminal:\n",
        "                        target_Qs_batch.append(rewards_mb[i])\n",
        "                        \n",
        "                    else:\n",
        "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
        "                        target_Qs_batch.append(target)\n",
        "                        \n",
        "\n",
        "                targets_mb = np.array([each for each in target_Qs_batch])\n",
        "\n",
        "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
        "                                        feed_dict={DQNetwork.inputs_: states_mb,\n",
        "                                                   DQNetwork.target_Q: targets_mb,\n",
        "                                                   DQNetwork.actions_: actions_mb})\n",
        "\n",
        "                # Write TF Summaries\n",
        "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
        "                                                       DQNetwork.target_Q: targets_mb,\n",
        "                                                       DQNetwork.actions_: actions_mb})\n",
        "                writer.add_summary(summary, episode)\n",
        "                writer.flush()\n",
        "\n",
        "            # Save model every 5 episodes\n",
        "            if episode % 5 == 0:\n",
        "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
        "                print(\"Model Saved\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 0 Total reward: 480.0 Explore P: 0.9904 Training Loss 9.9791\n",
            "Model Saved\n",
            "Episode: 1 Total reward: 120.0 Explore P: 0.9838 Training Loss 0.4405\n",
            "Episode: 2 Total reward: 300.0 Explore P: 0.9730 Training Loss 0.0982\n",
            "Episode: 3 Total reward: 120.0 Explore P: 0.9680 Training Loss 15.6547\n",
            "Episode: 4 Total reward: 180.0 Explore P: 0.9623 Training Loss 0.0636\n",
            "Episode: 5 Total reward: 50.0 Explore P: 0.9585 Training Loss 0.0112\n",
            "Model Saved\n",
            "Episode: 6 Total reward: 460.0 Explore P: 0.9448 Training Loss 0.0130\n",
            "Episode: 7 Total reward: 330.0 Explore P: 0.9351 Training Loss 21.3551\n",
            "Episode: 8 Total reward: 485.0 Explore P: 0.9243 Training Loss 0.0740\n",
            "Episode: 9 Total reward: 65.0 Explore P: 0.9182 Training Loss 0.1947\n",
            "Episode: 10 Total reward: 65.0 Explore P: 0.9124 Training Loss 13.9551\n",
            "Model Saved\n",
            "Episode: 11 Total reward: 110.0 Explore P: 0.9067 Training Loss 0.2223\n",
            "Episode: 12 Total reward: 415.0 Explore P: 0.8990 Training Loss 0.0094\n",
            "Episode: 13 Total reward: 155.0 Explore P: 0.8926 Training Loss 0.0391\n",
            "Episode: 14 Total reward: 210.0 Explore P: 0.8855 Training Loss 0.0994\n",
            "Episode: 15 Total reward: 160.0 Explore P: 0.8766 Training Loss 0.3865\n",
            "Model Saved\n",
            "Episode: 16 Total reward: 105.0 Explore P: 0.8712 Training Loss 0.0395\n",
            "Episode: 17 Total reward: 20.0 Explore P: 0.8667 Training Loss 0.0671\n",
            "Episode: 18 Total reward: 275.0 Explore P: 0.8594 Training Loss 647.6030\n",
            "Episode: 19 Total reward: 110.0 Explore P: 0.8518 Training Loss 0.0951\n",
            "Episode: 20 Total reward: 30.0 Explore P: 0.8483 Training Loss 9.5618\n",
            "Model Saved\n",
            "Episode: 21 Total reward: 80.0 Explore P: 0.8450 Training Loss 6.1844\n",
            "Episode: 22 Total reward: 65.0 Explore P: 0.8396 Training Loss 0.0037\n",
            "Episode: 23 Total reward: 215.0 Explore P: 0.8329 Training Loss 0.4045\n",
            "Episode: 24 Total reward: 230.0 Explore P: 0.8247 Training Loss 3.0524\n",
            "Episode: 25 Total reward: 70.0 Explore P: 0.8221 Training Loss 6.1874\n",
            "Model Saved\n",
            "Episode: 26 Total reward: 110.0 Explore P: 0.8181 Training Loss 8.3198\n",
            "Episode: 27 Total reward: 120.0 Explore P: 0.8127 Training Loss 1.5477\n",
            "Episode: 28 Total reward: 255.0 Explore P: 0.8049 Training Loss 0.0063\n",
            "Episode: 29 Total reward: 225.0 Explore P: 0.7970 Training Loss 3.0356\n",
            "Episode: 30 Total reward: 35.0 Explore P: 0.7917 Training Loss 6.1591\n",
            "Model Saved\n",
            "Episode: 31 Total reward: 60.0 Explore P: 0.7876 Training Loss 7.6427\n",
            "Episode: 32 Total reward: 45.0 Explore P: 0.7830 Training Loss 1.5419\n",
            "Episode: 33 Total reward: 105.0 Explore P: 0.7797 Training Loss 0.0673\n",
            "Episode: 34 Total reward: 90.0 Explore P: 0.7758 Training Loss 6.5127\n",
            "Episode: 35 Total reward: 80.0 Explore P: 0.7726 Training Loss 3.8131\n",
            "Model Saved\n",
            "Episode: 36 Total reward: 180.0 Explore P: 0.7663 Training Loss 3.4604\n",
            "Episode: 37 Total reward: 50.0 Explore P: 0.7622 Training Loss 0.0037\n",
            "Episode: 38 Total reward: 215.0 Explore P: 0.7529 Training Loss 0.0792\n",
            "Episode: 39 Total reward: 155.0 Explore P: 0.7483 Training Loss 19.1847\n",
            "Episode: 40 Total reward: 335.0 Explore P: 0.7425 Training Loss 1.5418\n",
            "Model Saved\n",
            "Episode: 41 Total reward: 185.0 Explore P: 0.7367 Training Loss 3.0468\n",
            "Episode: 42 Total reward: 195.0 Explore P: 0.7305 Training Loss 9.6704\n",
            "Episode: 43 Total reward: 135.0 Explore P: 0.7245 Training Loss 6.8701\n",
            "Episode: 44 Total reward: 20.0 Explore P: 0.7207 Training Loss 6.1558\n",
            "Episode: 45 Total reward: 95.0 Explore P: 0.7168 Training Loss 13.9072\n",
            "Model Saved\n",
            "Episode: 46 Total reward: 290.0 Explore P: 0.7066 Training Loss 0.0703\n",
            "Episode: 47 Total reward: 80.0 Explore P: 0.7027 Training Loss 7.6629\n",
            "Episode: 48 Total reward: 50.0 Explore P: 0.6988 Training Loss 6.9277\n",
            "Episode: 49 Total reward: 70.0 Explore P: 0.6947 Training Loss 9.6272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "teHm6Y1FMStS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "f76e23e9-1397-43a6-8105-3c84a0682048"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    total_test_rewards = []\n",
        "    \n",
        "    # Load the model\n",
        "    saver.restore(sess, \"./models/model.ckpt\")\n",
        "    \n",
        "    for episode in range(1):\n",
        "        total_rewards = 0\n",
        "        \n",
        "        state = env.reset()\n",
        "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "        \n",
        "        print(\"****************************************************\")\n",
        "        print(\"EPISODE \", episode)\n",
        "        \n",
        "        while True:\n",
        "            # Reshape the state\n",
        "            state = state.reshape((1, *state_size))\n",
        "            # Get action from Q-network \n",
        "            # Estimate the Qs values state\n",
        "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state})\n",
        "            \n",
        "            # Take the biggest Q value (= the best action)\n",
        "            choice = np.argmax(Qs)\n",
        "            action = possible_actions[choice]\n",
        "            \n",
        "            #Perform the action and get the next_state, reward, and done information\n",
        "            next_state, reward, done, _ = env.step(np.argmax(action))\n",
        "#             env.render()\n",
        "            \n",
        "            total_rewards += reward\n",
        "\n",
        "            if done:\n",
        "                print (\"Score\", total_rewards)\n",
        "                total_test_rewards.append(total_rewards)\n",
        "                break\n",
        "                \n",
        "                \n",
        "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "            state = next_state\n",
        "            \n",
        "    env.close()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
            "****************************************************\n",
            "EPISODE  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Score 30.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pph-Vp6jcHHK",
        "colab_type": "code",
        "outputId": "4409efe7-13c6-44f3-bbf6-c075ea06ca77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('SpaceInvaders-v0')\n",
        "obs=env.reset()\n",
        "# print (obs.shape)\n",
        "# print (obs)\n",
        "# obs=rgb2gray(obs)\n",
        "# io.imshow(obs)\n",
        "for _ in range(15):\n",
        "    time.sleep(0.3)\n",
        "    io.imshow(obs)\n",
        "#     print (obs)\n",
        "#     cv2.imshow(\"blah\",obs)\n",
        "#     plt.imshow(obs)\n",
        "#     env.render()\n",
        "    obs,rew,done,info=env.step(env.action_space.sample()) # take a random action"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/io/_plugins/matplotlib_plugin.py:51: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  out_of_range_float = (np.issubdtype(image.dtype, np.float) and\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py:1334: MatplotlibDeprecationWarning: \n",
            "box-forced\n",
            "  \"2.2\", \"box-forced\", obj_type=\"keyword argument\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATAAAAGACAYAAAAqMT6GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMb6X4eSdMK76pmtAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "zjmAVOI8hsNX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}